{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(64, input_shape=(1, 192)))  # input_shape=(timesteps, features)\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate some dummy data for training and testing\n",
        "x_train = np.random.rand(700, 1, 192)\n",
        "y_train = np.random.randint(0, 10, size=(700, 1))\n",
        "x_test = np.random.rand(200, 1, 192)\n",
        "y_test = np.random.randint(0, 10, size=(200, 1))\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y3HJtKGa9Tb",
        "outputId": "21fb81d9-efe1-4d37-dd01-db4d5a27236d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "22/22 [==============================] - 3s 32ms/step - loss: 2.3090 - accuracy: 0.1171 - val_loss: 2.3040 - val_accuracy: 0.0900\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2948 - accuracy: 0.1357 - val_loss: 2.3064 - val_accuracy: 0.1000\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2873 - accuracy: 0.1343 - val_loss: 2.3049 - val_accuracy: 0.1350\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2759 - accuracy: 0.1614 - val_loss: 2.3082 - val_accuracy: 0.1000\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2597 - accuracy: 0.1671 - val_loss: 2.3107 - val_accuracy: 0.0900\n",
            "Accuracy: 9.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Pad the sequences to have a maximum length of 200\n",
        "x_train = pad_sequences(x_train, maxlen=200)\n",
        "x_test = pad_sequences(x_test, maxlen=200)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=128, input_length=200))\n",
        "model.add(tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "model.add(tf.keras.layers.LSTM(64, dropout=0.2))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy: %.2f%%' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn6MCanafI5_",
        "outputId": "7a4e4fd6-ddfb-4388-bfce-f6c636f72908"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 110s 137ms/step - loss: 0.3701 - accuracy: 0.8259 - val_loss: 0.2840 - val_accuracy: 0.8808\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 105s 134ms/step - loss: 0.2047 - accuracy: 0.9220 - val_loss: 0.3001 - val_accuracy: 0.8797\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 105s 135ms/step - loss: 0.1346 - accuracy: 0.9517 - val_loss: 0.3563 - val_accuracy: 0.8700\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 103s 132ms/step - loss: 0.0797 - accuracy: 0.9738 - val_loss: 0.4431 - val_accuracy: 0.8631\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 106s 136ms/step - loss: 0.0510 - accuracy: 0.9837 - val_loss: 0.4999 - val_accuracy: 0.8682\n",
            "Accuracy: 86.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "   # The input data is first embedded into a dense vector space using the Embedding layer.\n",
        "    The embedded data is then fed into a Conv1D layer, which applies a 1D convolutional filter to extract local features from the sequence.\n",
        "    The output from the Conv1D layer is then fed into a MaxPooling1D layer, which downsamples the features to reduce the spatial dimensions.\n",
        "    The output from the MaxPooling1D layer is then fed into an LSTM layer, which models the temporal dependencies in the sequence.\n",
        "    The output from the LSTM layer is then fed into a Dense layer with a sigmoid activation function, which outputs a probability score.\n",
        "\n",
        "This model combines the strengths of both CNNs and LSTMs:\n",
        "\n",
        "    The CNN extracts local features from the sequence, which can capture sentiment-bearing phrases or words.\n",
        "    The LSTM models the temporal dependencies in the sequence, which can capture the context and relationships between words.\n",
        "\n",
        "By combining these two architectures, the model can learn to extract both local and global features from the sequence, leading to improved sentiment analysis performance.\n",
        "\n",
        "this is a CNN-LSTM model, and you may need to tune the hyperparameters or experiment with different architectures to achieve the best results for your specific task.\n",
        "\n"
      ],
      "metadata": {
        "id": "LpPJjOKShrvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}